{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-01-03T04:18:05.730950Z",
     "iopub.status.busy": "2021-01-03T04:18:05.730326Z",
     "iopub.status.idle": "2021-01-03T04:18:05.755551Z",
     "shell.execute_reply": "2021-01-03T04:18:05.754914Z"
    },
    "papermill": {
     "duration": 0.038349,
     "end_time": "2021-01-03T04:18:05.755668",
     "exception": false,
     "start_time": "2021-01-03T04:18:05.717319",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/chenleo/model_42_3_finetune.hdf5\n",
      "/kaggle/input/chenleo/encoder.hdf5\n",
      "/kaggle/input/chenleo/model_42_0.hdf5\n",
      "/kaggle/input/chenleo/model_42_0_finetune.hdf5\n",
      "/kaggle/input/chenleo/model_42_4_finetune.hdf5\n",
      "/kaggle/input/chenleo/model_42_3.hdf5\n",
      "/kaggle/input/chenleo/model_42_2.hdf5\n",
      "/kaggle/input/chenleo/model_42_1_finetune.hdf5\n",
      "/kaggle/input/chenleo/model_42_1.hdf5\n",
      "/kaggle/input/chenleo/model_42_2_finetune.hdf5\n",
      "/kaggle/input/chenleo/best_hp_42.pkl\n",
      "/kaggle/input/chenleo/model_42_4.hdf5\n",
      "/kaggle/input/jane-street-market-prediction/example_sample_submission.csv\n",
      "/kaggle/input/jane-street-market-prediction/features.csv\n",
      "/kaggle/input/jane-street-market-prediction/example_test.csv\n",
      "/kaggle/input/jane-street-market-prediction/train.csv\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/competition.cpython-37m-x86_64-linux-gnu.so\n",
      "/kaggle/input/jane-street-market-prediction/janestreet/__init__.py\n",
      "/kaggle/input/chenleo123/model_123_0_finetune.hdf5\n",
      "/kaggle/input/chenleo123/encoder_123.hdf5\n",
      "/kaggle/input/chenleo123/model_123_1_finetune.hdf5\n",
      "/kaggle/input/chenleo123/model_123_2.hdf5\n",
      "/kaggle/input/chenleo123/model_123_4.hdf5\n",
      "/kaggle/input/chenleo123/model_123_4_finetune.hdf5\n",
      "/kaggle/input/chenleo123/model_123_2_finetune.hdf5\n",
      "/kaggle/input/chenleo123/model_123_3_finetune.hdf5\n",
      "/kaggle/input/chenleo123/model_123_1.hdf5\n",
      "/kaggle/input/chenleo123/best_hp_123.pkl\n",
      "/kaggle/input/chenleo123/model_123_0.hdf5\n",
      "/kaggle/input/chenleo123/model_123_3.hdf5\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2021-01-03T04:18:05.778487Z",
     "iopub.status.busy": "2021-01-03T04:18:05.777729Z",
     "iopub.status.idle": "2021-01-03T04:18:12.933454Z",
     "shell.execute_reply": "2021-01-03T04:18:12.932970Z"
    },
    "papermill": {
     "duration": 7.168589,
     "end_time": "2021-01-03T04:18:12.933580",
     "exception": false,
     "start_time": "2021-01-03T04:18:05.764991",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Dropout, Concatenate, Lambda, GaussianNoise, Activation\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from tqdm import tqdm\n",
    "from random import choices\n",
    "\n",
    "\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:18:12.950653Z",
     "iopub.status.busy": "2021-01-03T04:18:12.950092Z",
     "iopub.status.idle": "2021-01-03T04:18:12.968054Z",
     "shell.execute_reply": "2021-01-03T04:18:12.968544Z"
    },
    "papermill": {
     "duration": 0.027877,
     "end_time": "2021-01-03T04:18:12.968696",
     "exception": false,
     "start_time": "2021-01-03T04:18:12.940819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection._split import _BaseKFold, indexable, _num_samples\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "# modified code for group gaps; source\n",
    "# https://github.com/getgaurav2/scikit-learn/blob/d4a3af5cc9da3a76f0266932644b884c99724c57/sklearn/model_selection/_split.py#L2243\n",
    "class PurgedGroupTimeSeriesSplit(_BaseKFold):\n",
    "  \n",
    "\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,\n",
    "                 n_splits=5,\n",
    "                 *,\n",
    "                 max_train_group_size=np.inf,\n",
    "                 max_test_group_size=np.inf,\n",
    "                 group_gap=None,\n",
    "                 verbose=False\n",
    "                 ):\n",
    "        super().__init__(n_splits, shuffle=False, random_state=None)\n",
    "        self.max_train_group_size = max_train_group_size\n",
    "        self.group_gap = group_gap\n",
    "        self.max_test_group_size = max_test_group_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def split(self, X, y=None, groups=None):\n",
    "       \n",
    "        if groups is None:\n",
    "            raise ValueError(\n",
    "                )\n",
    "        X, y, groups = indexable(X, y, groups)\n",
    "        n_samples = _num_samples(X)\n",
    "        n_splits = self.n_splits\n",
    "        group_gap = self.group_gap\n",
    "        max_test_group_size = self.max_test_group_size\n",
    "        max_train_group_size = self.max_train_group_size\n",
    "        n_folds = n_splits + 1\n",
    "        group_dict = {}\n",
    "        u, ind = np.unique(groups, return_index=True)\n",
    "        unique_groups = u[np.argsort(ind)]\n",
    "        n_samples = _num_samples(X)\n",
    "        n_groups = _num_samples(unique_groups)\n",
    "        for idx in np.arange(n_samples):\n",
    "            if (groups[idx] in group_dict):\n",
    "                group_dict[groups[idx]].append(idx)\n",
    "            else:\n",
    "                group_dict[groups[idx]] = [idx]\n",
    "        if n_folds > n_groups:\n",
    "            raise ValueError(\n",
    "                (\"Cannot have number of folds={0} greater than\"\n",
    "                 \" the number of groups={1}\").format(n_folds,\n",
    "                                                     n_groups))\n",
    "\n",
    "        group_test_size = min(n_groups // n_folds, max_test_group_size)\n",
    "        group_test_starts = range(n_groups - n_splits * group_test_size,\n",
    "                                  n_groups, group_test_size)\n",
    "        for group_test_start in group_test_starts:\n",
    "            train_array = []\n",
    "            test_array = []\n",
    "\n",
    "            group_st = max(0, group_test_start - group_gap - max_train_group_size)\n",
    "            for train_group_idx in unique_groups[group_st:(group_test_start - group_gap)]:\n",
    "                train_array_tmp = group_dict[train_group_idx]\n",
    "                \n",
    "                train_array = np.sort(np.unique(\n",
    "                                      np.concatenate((train_array,\n",
    "                                                      train_array_tmp)),\n",
    "                                      axis=None), axis=None)\n",
    "\n",
    "            train_end = train_array.size\n",
    " \n",
    "            for test_group_idx in unique_groups[group_test_start:\n",
    "                                                group_test_start +\n",
    "                                                group_test_size]:\n",
    "                test_array_tmp = group_dict[test_group_idx]\n",
    "                test_array = np.sort(np.unique(\n",
    "                                              np.concatenate((test_array,\n",
    "                                                              test_array_tmp)),\n",
    "                                     axis=None), axis=None)\n",
    "\n",
    "            test_array  = test_array[group_gap:]\n",
    "            \n",
    "            \n",
    "            if self.verbose > 0:\n",
    "                    pass\n",
    "                    \n",
    "            yield [int(i) for i in train_array], [int(i) for i in test_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:18:12.986260Z",
     "iopub.status.busy": "2021-01-03T04:18:12.985634Z",
     "iopub.status.idle": "2021-01-03T04:18:12.995487Z",
     "shell.execute_reply": "2021-01-03T04:18:12.995986Z"
    },
    "papermill": {
     "duration": 0.020176,
     "end_time": "2021-01-03T04:18:12.996135",
     "exception": false,
     "start_time": "2021-01-03T04:18:12.975959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CVTuner(kt.engine.tuner.Tuner):\n",
    "    def run_trial(self, trial, X, y, splits, batch_size=32, epochs=1,callbacks=None):\n",
    "        val_losses = []\n",
    "        for train_indices, test_indices in splits:\n",
    "            X_train, X_test = [x[train_indices] for x in X], [x[test_indices] for x in X]\n",
    "            y_train, y_test = [a[train_indices] for a in y], [a[test_indices] for a in y]\n",
    "            if len(X_train) < 2:\n",
    "                X_train = X_train[0]\n",
    "                X_test = X_test[0]\n",
    "            if len(y_train) < 2:\n",
    "                y_train = y_train[0]\n",
    "                y_test = y_test[0]\n",
    "            \n",
    "            model = self.hypermodel.build(trial.hyperparameters)\n",
    "            hist = model.fit(X_train,y_train,\n",
    "                      validation_data=(X_test,y_test),\n",
    "                      epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                      callbacks=callbacks)\n",
    "            \n",
    "            val_losses.append([hist.history[k][-1] for k in hist.history])\n",
    "        val_losses = np.asarray(val_losses)\n",
    "        self.oracle.update_trial(trial.trial_id, {k:np.mean(val_losses[:,i]) for i,k in enumerate(hist.history.keys())})\n",
    "        self.save_model(trial.trial_id, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:18:13.014344Z",
     "iopub.status.busy": "2021-01-03T04:18:13.013749Z",
     "iopub.status.idle": "2021-01-03T04:20:37.392403Z",
     "shell.execute_reply": "2021-01-03T04:20:37.391746Z"
    },
    "papermill": {
     "duration": 144.388981,
     "end_time": "2021-01-03T04:20:37.392529",
     "exception": false,
     "start_time": "2021-01-03T04:18:13.003548",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "USE_FINETUNE = True     \n",
    "FOLDS = 4\n",
    "SEED = 42\n",
    "\n",
    "train = pd.read_csv('../input/jane-street-market-prediction/train.csv')\n",
    "train = train.query('date > 85').reset_index(drop = True) \n",
    "train = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n",
    "train.fillna(train.mean(),inplace=True)\n",
    "train = train.query('weight > 0').reset_index(drop = True)\n",
    "#train['action'] = (train['resp'] > 0).astype('int')\n",
    "train['action'] =  (  (train['resp_1'] >  0.00001 ) & (train['resp_2'] > 0.00001 ) & (train['resp_3'] >  0.00001) & (train['resp_4'] >  0.00001 ) &  (train['resp'] >  0.00001 )   ).astype('int')\n",
    "features = [c for c in train.columns if 'feature' in c]\n",
    "\n",
    "resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n",
    "\n",
    "X = train[features].values\n",
    "y = np.stack([(train[c] > 0.000001).astype('int') for c in resp_cols]).T #Multitarget\n",
    "\n",
    "f_mean = np.mean(train[features[1:]].values,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:20:37.416009Z",
     "iopub.status.busy": "2021-01-03T04:20:37.415389Z",
     "iopub.status.idle": "2021-01-03T04:20:37.418181Z",
     "shell.execute_reply": "2021-01-03T04:20:37.418632Z"
    },
    "papermill": {
     "duration": 0.01855,
     "end_time": "2021-01-03T04:20:37.418760",
     "exception": false,
     "start_time": "2021-01-03T04:20:37.400210",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_autoencoder(input_dim,output_dim,noise=0.05):\n",
    "    i = Input(input_dim)\n",
    "    encoded = BatchNormalization()(i)\n",
    "    encoded = GaussianNoise(noise)(encoded)\n",
    "    encoded = Dense(640,activation='relu')(encoded)\n",
    "    decoded = Dropout(0.2)(encoded)\n",
    "    decoded = Dense(input_dim,name='decoded')(decoded)\n",
    "    x = Dense(320,activation='relu')(decoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(output_dim,activation='sigmoid',name='label_output')(x)\n",
    "    \n",
    "    encoder = Model(inputs=i,outputs=encoded)\n",
    "    autoencoder = Model(inputs=i,outputs=[decoded,x])\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(0.001),loss={'decoded':'mse','label_output':'binary_crossentropy'})\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:20:37.443593Z",
     "iopub.status.busy": "2021-01-03T04:20:37.442940Z",
     "iopub.status.idle": "2021-01-03T04:20:37.446000Z",
     "shell.execute_reply": "2021-01-03T04:20:37.445405Z"
    },
    "papermill": {
     "duration": 0.019972,
     "end_time": "2021-01-03T04:20:37.446106",
     "exception": false,
     "start_time": "2021-01-03T04:20:37.426134",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(hp,input_dim,output_dim,encoder):\n",
    "    inputs = Input(input_dim)\n",
    "    \n",
    "    x = encoder(inputs)\n",
    "    x = Concatenate()([x,inputs]) #use both raw and encoded features\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(hp.Float('init_dropout',0.0,0.5))(x)\n",
    "    \n",
    "    for i in range(hp.Int('num_layers',1,5)):\n",
    "        x = Dense(hp.Int('num_units_{i}',64,256))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Lambda(tf.keras.activations.swish)(x)\n",
    "        x = Dropout(hp.Float(f'dropout_{i}',0.0,0.5))(x)\n",
    "    x = Dense(output_dim,activation='sigmoid')(x)\n",
    "    model = Model(inputs=inputs,outputs=x)\n",
    "    model.compile(optimizer=Adam(hp.Float('lr',0.00001,0.1,default=0.001)),loss=BinaryCrossentropy(label_smoothing=hp.Float('label_smoothing',0.0,0.1)),metrics=[tf.keras.metrics.AUC(name = 'auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:20:37.467719Z",
     "iopub.status.busy": "2021-01-03T04:20:37.467076Z",
     "iopub.status.idle": "2021-01-03T04:20:37.806586Z",
     "shell.execute_reply": "2021-01-03T04:20:37.806061Z"
    },
    "papermill": {
     "duration": 0.353162,
     "end_time": "2021-01-03T04:20:37.806691",
     "exception": false,
     "start_time": "2021-01-03T04:20:37.453529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "autoencoder, encoder = create_autoencoder(X.shape[-1],y.shape[-1],noise=0.1)\n",
    "if TRAINING:\n",
    "    autoencoder.fit(X,(X,y),\n",
    "                    epochs=1002,\n",
    "                    batch_size=4096, \n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[EarlyStopping('val_loss',patience=10,restore_best_weights=True)])\n",
    "    encoder.save_weights('./encoder.hdf5')\n",
    "else:\n",
    "    encoder.load_weights('../input/chenleo/encoder.hdf5')\n",
    "encoder.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:20:37.835676Z",
     "iopub.status.busy": "2021-01-03T04:20:37.835021Z",
     "iopub.status.idle": "2021-01-03T04:20:37.837495Z",
     "shell.execute_reply": "2021-01-03T04:20:37.837902Z"
    },
    "papermill": {
     "duration": 0.023499,
     "end_time": "2021-01-03T04:20:37.838038",
     "exception": false,
     "start_time": "2021-01-03T04:20:37.814539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LiteModel:\n",
    "    \n",
    "    @classmethod\n",
    "    def from_file(cls, model_path):\n",
    "        return LiteModel(tf.lite.Interpreter(model_path=model_path))\n",
    "    \n",
    "    @classmethod\n",
    "    def from_keras_model(cls, kmodel):\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(kmodel)\n",
    "        tflite_model = converter.convert()\n",
    "        return LiteModel(tf.lite.Interpreter(model_content=tflite_model))\n",
    "    \n",
    "    def __init__(self, interpreter):\n",
    "        self.interpreter = interpreter\n",
    "        self.interpreter.allocate_tensors()\n",
    "        input_det = self.interpreter.get_input_details()[0]\n",
    "        output_det = self.interpreter.get_output_details()[0]\n",
    "        self.input_index = input_det[\"index\"]\n",
    "        self.output_index = output_det[\"index\"]\n",
    "        self.input_shape = input_det[\"shape\"]\n",
    "        self.output_shape = output_det[\"shape\"]\n",
    "        self.input_dtype = input_det[\"dtype\"]\n",
    "        self.output_dtype = output_det[\"dtype\"]\n",
    "        \n",
    "    def predict(self, inp):\n",
    "        inp = inp.astype(self.input_dtype)\n",
    "        count = inp.shape[0]\n",
    "        out = np.zeros((count, self.output_shape[1]), dtype=self.output_dtype)\n",
    "        for i in range(count):\n",
    "            self.interpreter.set_tensor(self.input_index, inp[i:i+1])\n",
    "            self.interpreter.invoke()\n",
    "            out[i] = self.interpreter.get_tensor(self.output_index)[0]\n",
    "        return out\n",
    "    \n",
    "    def predict_single(self, inp):\n",
    "        \"\"\" Like predict(), but only for a single record. The input data can be a Python list. \"\"\"\n",
    "        inp = np.array([inp], dtype=self.input_dtype)\n",
    "        self.interpreter.set_tensor(self.input_index, inp)\n",
    "        self.interpreter.invoke()\n",
    "        out = self.interpreter.get_tensor(self.output_index)\n",
    "        return out[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:20:37.867972Z",
     "iopub.status.busy": "2021-01-03T04:20:37.867320Z",
     "iopub.status.idle": "2021-01-03T04:20:54.142127Z",
     "shell.execute_reply": "2021-01-03T04:20:54.142641Z"
    },
    "papermill": {
     "duration": 16.297159,
     "end_time": "2021-01-03T04:20:54.142798",
     "exception": false,
     "start_time": "2021-01-03T04:20:37.845639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_fn = lambda hp: create_model(hp,X.shape[-1],y.shape[-1],encoder)\n",
    "\n",
    "tuner = CVTuner(\n",
    "        hypermodel=model_fn,\n",
    "        oracle=kt.oracles.BayesianOptimization(\n",
    "        objective= kt.Objective('val_auc', direction='max'),\n",
    "        num_initial_points=4,\n",
    "        max_trials=60))\n",
    "\n",
    "FOLDS = 5\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "if TRAINING:\n",
    "    gkf = PurgedGroupTimeSeriesSplit(n_splits = FOLDS, group_gap=20)\n",
    "    splits = list(gkf.split(y, groups=train['date'].values))\n",
    "    tuner.search((X,),(y,),splits=splits,batch_size=16384,epochs=300,callbacks=[EarlyStopping('val_auc', mode='max',patience=3)])\n",
    "    hp  = tuner.get_best_hyperparameters(1)[0]\n",
    "    pd.to_pickle(hp,f'./best_hp_{SEED}.pkl')\n",
    "    for fold, (train_indices, test_indices) in enumerate(splits):\n",
    "        model = model_fn(hp)\n",
    "        X_train, X_test = X[train_indices], X[test_indices]\n",
    "        y_train, y_test = y[train_indices], y[test_indices]\n",
    "        model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=300,batch_size=16384,callbacks=[EarlyStopping('val_auc',mode='max',patience=10,restore_best_weights=True)])\n",
    "        model.save_weights(f'./model_{SEED}_{fold}.hdf5')\n",
    "        model.compile(Adam(hp.get('lr')/100),loss='binary_crossentropy')\n",
    "        model.fit(X_test,y_test,epochs=6,batch_size=16384)\n",
    "        model.save_weights(f'./model_{SEED}_{fold}_finetune.hdf5')\n",
    "    tuner.results_summary()\n",
    "else:\n",
    "    models = []\n",
    "    hp = pd.read_pickle(f'../input/chenleo/best_hp_{SEED}.pkl')\n",
    "    for f in range(FOLDS):\n",
    "        model = model_fn(hp)\n",
    "        if USE_FINETUNE:\n",
    "            model.load_weights(f'../input/chenleo/model_{SEED}_{f}_finetune.hdf5')\n",
    "        else:\n",
    "            model.load_weights(f'../input/chenleo/model_{SEED}_{f}.hdf5')\n",
    "        model = LiteModel.from_keras_model(model)\n",
    "        models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-03T04:20:54.167786Z",
     "iopub.status.busy": "2021-01-03T04:20:54.167192Z",
     "iopub.status.idle": "2021-01-03T04:23:19.127724Z",
     "shell.execute_reply": "2021-01-03T04:23:19.128283Z"
    },
    "papermill": {
     "duration": 144.977249,
     "end_time": "2021-01-03T04:23:19.128448",
     "exception": false,
     "start_time": "2021-01-03T04:20:54.151199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15219it [02:24, 105.01it/s]\n"
     ]
    }
   ],
   "source": [
    "if not  TRAINING:\n",
    "    import janestreet\n",
    "    env = janestreet.make_env()\n",
    "    th = 0.5\n",
    "    for (test_df, pred_df) in tqdm(env.iter_test()):\n",
    "        if test_df['weight'].item() > 0:\n",
    "            x_tt = test_df.loc[:, features].values\n",
    "            if np.isnan(x_tt[:, 1:].sum()):\n",
    "                x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n",
    "            pred = np.mean([model.predict(x_tt) for model in models],axis=0)\n",
    "            pred = np.mean(pred)\n",
    "            pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n",
    "        else:\n",
    "            pred_df.action = 0\n",
    "        env.predict(pred_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 318.611735,
   "end_time": "2021-01-03T04:23:19.672040",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-03T04:18:01.060305",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
